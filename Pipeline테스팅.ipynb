{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfb92c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\PaperGraph\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4f0c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:57,835 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:57,849 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:57,863 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:57,864 [RapidOCR] main.py:50: Using C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,408 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,409 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,412 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,412 [RapidOCR] main.py:50: Using C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,569 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,570 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,591 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-21 19:29:58,592 [RapidOCR] main.py:50: Using C:\\Anaconda3\\envs\\PaperGraph\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n"
     ]
    }
   ],
   "source": [
    "sourece = \"ReAct.pdf\"\n",
    "loader = DoclingLoader(\n",
    "    file_path=sourece,\n",
    "    export_type = \"markdown\"\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f41106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## REACT : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\n",
      "\n",
      "Shunyu Yao ∗ *,1 , Jeffrey Zhao 2 , Dian Yu 2 , Nan Du 2 , Izhak Shafran 2 , Karthik Narasimhan 1 , Yuan Cao 2\n",
      "\n",
      "1 Department of Computer Science, Princeton University 2 Google Research, Brain team\n",
      "\n",
      "1 {shunyuy,karthikn}@princeton.edu 2 {jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive deci\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content[:500])  # Print the first 500 characters of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15a1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Docling 마크다운 출력용 섹션 단위 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 청크 최대 크기: 1200자\n",
    "    # - 영어 논문 1200자 ≈ 300토큰 (대부분 임베딩 모델 512토큰 제한 내)\n",
    "    # - 논문 섹션(Abstract, Introduction) 전체를 한 청크로 유지 가능\n",
    "    # - 너무 작으면 문맥 손실, 너무 크면 검색 정확도 하락\n",
    "    chunk_size=1200,\n",
    "    \n",
    "    # 청크 간 겹치는 부분: 200자 (chunk_size의 16%)\n",
    "    # - 섹션 경계에서 문맥이 끊기는 것 방지\n",
    "    # - 검색 시 경계 부근 정보 놓치지 않도록\n",
    "    # - 일반적으로 chunk_size의 10-20% 권장\n",
    "    chunk_overlap=200,\n",
    "    \n",
    "    # 분할 우선순위 (위에서부터 순서대로 시도)\n",
    "    separators=[\n",
    "        \"\\n## \",      # 1순위: 논문 섹션 헤더 (## Abstract, ## Introduction)\n",
    "                      # Docling이 주요 섹션을 2레벨 헤더로 변환하므로\n",
    "                      # 가장 큰 의미 단위인 섹션을 먼저 보존\n",
    "        \n",
    "        \"\\n### \",     # 2순위: 서브섹션 (### 3.1 Dataset, ### 3.2 Model)\n",
    "                      # 섹션이 1200자 초과 시 서브섹션 단위로 자연스럽게 분할\n",
    "        \n",
    "        \"\\n\\n\",       # 3순위: 문단 경계\n",
    "                      # 서브섹션도 길면 문단 단위로 분할\n",
    "                      # 논문은 문단이 논리적 단위이므로 여기서 자르는 게 자연스러움\n",
    "        \n",
    "        \"\\n\",         # 4순위: 줄바꿈\n",
    "                      # 문단도 길면 줄 단위로\n",
    "                      # 수식, 리스트, 코드 블록 등은 줄바꿈으로 구분됨\n",
    "        \n",
    "        \". \",         # 5순위: 문장 끝 (마침표 + 공백)\n",
    "                      # 최소한 완전한 문장은 유지\n",
    "                      # 공백까지 포함해야 문장 끝 정확히 감지\n",
    "        \n",
    "        \" \",          # 6순위: 단어 경계 (공백)\n",
    "                      # 문장도 초과하면 단어 단위로\n",
    "                      # 최소한 단어는 쪼개지지 않게\n",
    "        \n",
    "        \"\"            # 7순위: 강제 분할 (문자 단위)\n",
    "                      # 긴 URL, 코드, 수식 등 chunk_size 초과 시 문자로 강제 분할\n",
    "                      # 에러 방지용 최후의 안전장치\n",
    "    ],\n",
    "    \n",
    "    # 원본 문서에서의 시작 위치를 메타데이터에 추가\n",
    "    # - RAG 검색 후 출처 추적 가능 (몇 번째 문자/페이지에서 왔는지)\n",
    "    # - chunk.metadata['start_index']로 접근\n",
    "    # - 사용자에게 \"논문 3페이지 Introduction 섹션\" 같은 정보 제공 가능\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1ba177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct , to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness' metadata={'source': 'ReAct.pdf', 'start_index': 365}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaperGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
